from preprocessing import CustomStandardScaler
import sys
import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('gtk3agg')
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler

def import_csv(path):
    return pd.read_csv(path)

class LogisticRegressionOVR:
    def __init__(self, learning_rate=0.01, max_iter=1000, class_encoder=None):
        """
        Initialise le mod√®le avec un taux d'apprentissage et un nombre d'it√©rations pour la descente de gradient.
        """
        self.claas_encoder = class_encoder or {}
        self.learning_rate = learning_rate
        self.max_iter = max_iter
        self.thetas = []  # Liste pour stocker les poids de chaque mod√®le
        self.classes_ = None  # Stocker les noms des classes
        self.log_loss = []

    def sigmoid(self, z):
        """
        Fonction sigmo√Øde : transforme n'importe quelle valeur en un score entre 0 et 1.
        """
        return 1 / (1 + np.exp(-z))

    def cost_function(self, X, y, theta):
        """
        Fonction de co√ªt : mesure l'erreur entre les pr√©dictions et les vraies valeurs.
        """
        m = len(y)
        h = self.sigmoid(X @ theta)
        return (-1 / m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))

    def gradient_descent(self, X, y, theta, class_name):
        """
        Applique la descente de gradient pour trouver les poids optimaux.
        """
        m = len(y)
        log_loss_class = []
        for _ in range(self.max_iter):
            h = self.sigmoid(X @ theta)
            gradient = (1 / m) * X.T @ (h - y)  # Gradient de la fonction de co√ªt
            theta -= self.learning_rate * gradient  # Mise √† jour des poids
            log_loss_class.append(self.cost_function(X, y, theta))
        self.log_loss.append(log_loss_class)
        return theta

    def fit(self, X, y):
        """
        Entra√Æne le mod√®le en utilisant la m√©thode One-vs-All.
        """
        m, n = X.shape
        self.classes_ = np.unique(y)  # Liste des classes uniques
        self.thetas = np.zeros((len(self.classes_), n + 1))  # Initialisation des poids
        X_bias = np.c_[np.ones((m, 1)), X]  # Ajout d'un biais (colonne de 1)

        for i, c in enumerate(self.classes_):
            y_binary = (y == c).astype(int)  # Convertir en probl√®me binaire (1 si c'est la classe, sinon 0)
            theta = np.zeros(n + 1)  # Initialiser les poids
            self.thetas[i] = self.gradient_descent(X_bias, y_binary, theta, c)  # Entra√Æner le mod√®le

    def predict_proba(self, X):
        """
        Retourne les probabilit√©s pour chaque classe.
        """
        X_bias = np.c_[np.ones((X.shape[0], 1)), X]  # Ajouter le biais
        return self.sigmoid(X_bias @ self.thetas.T)  # Matrice de probabilit√©s

    def predict(self, X):
        """
        Pr√©dit la classe en prenant celle avec la plus grande probabilit√©.
        """
        probabilities = self.predict_proba(X)
        return self.classes_[np.argmax(probabilities, axis=1)]  # Retourne la classe avec la proba max

    def plot_log_loss(self):
        """
        Affiche la courbe de la fonction de co√ªt pour chaque classe.
        """
        for i, c in enumerate(self.classes_):
            plt.plot(self.log_loss[i], label=c)
        plt.title('Log Loss')
        plt.xlabel('Iterations')
        plt.ylabel('Cost')
        plt.legend()
        plt.show()

    def export_model(self, path):
        """
        Exporte le mod√®le pour une utilisation ult√©rieure.
        """
        with open(path, 'w') as f:
            for i, theta in enumerate(self.thetas):
                f.write(f'{self.classes_[i]}:{",".join(map(str, theta))}\n')

if __name__ == '__main__':
    if len(sys.argv) < 2:
        print('Please provide a path to the dataset')
        sys.exit(1)
    path = sys.argv[1]
    df = import_csv(path)
    df.drop(columns=['Index'], inplace=True)

    normalize = CustomStandardScaler()
    model = LogisticRegressionOVR(learning_rate=0.1, max_iter=10000)
    std = StandardScaler()

    feature_name = ['Astronomy',
                    'Herbology',
                    'Defense Against the Dark Arts',
                    'Divination',
                    'Muggle Studies',
                    'Ancient Runes',
                    'History of Magic',
                    'Transfiguration',
                    'Charms',
                    'Flying']

    # feature_name = df.select_dtypes(include=['float64', 'int64']).columns.tolist()
    target_name = ['Hogwarts House']

    df.dropna(axis=0, inplace=True)
    df.drop_duplicates(inplace=True)

    features = df[feature_name]
    target = df[target_name]

    # features = normalize.fit_transform(features)
    X = std.fit_transform(features)
    # normalize.save('normalizer.csv')

    # X = features.to_numpy()
    y = target.to_numpy().ravel()

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f'Accuracy: {accuracy} üéØ')
    model.export_model('model.txt')
    model.plot_log_loss()
